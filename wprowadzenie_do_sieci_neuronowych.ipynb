{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mg:\\BR\\Knime\\Anna Karpińska\\wprowadzenie_do_sieci_neuronowych.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error, classification_report, f1_score, roc_auc_score, mean_absolute_error\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_boston\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_iris\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[1;32mc:\\Users\\akarpinska2\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mload_boston\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    106\u001b[0m     msg \u001b[39m=\u001b[39m textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[0;32m    107\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, classification_report, f1_score, roc_auc_score, mean_absolute_error\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''przykład 1 - klasyfikacja kwiatów irysa\n",
    "\n",
    "    1. Za pomocą pakietu scikit-learn załadujemy dane ze zbioru IRIS.\n",
    "    2. Przekonwertujemy je do postaci Data Frame, żeby móc wygodnie operować na danych.\n",
    "    3. Przeprowadzimy podział na zbiór treningowy i testowy.\n",
    "    4. Zamienimy wektor wartości oczekiwanych (\"Y\") na format one-hot encoding, żeby zrozumieć zasadę działania funkcji Softmax.\n",
    "    5. Przprowadzimy normalizację danych.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\BR\\Knime\\Anna Karpińska\\wprowadzenie_do_sieci_neuronowych.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m iris \u001b[39m=\u001b[39m load_iris()\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m iris_X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(iris\u001b[39m.\u001b[39mdata, columns\u001b[39m=\u001b[39miris\u001b[39m.\u001b[39mfeature_names)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/BR/Knime/Anna%20Karpi%C5%84ska/wprowadzenie_do_sieci_neuronowych.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m iris_Y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(iris\u001b[39m.\u001b[39mtarget)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_iris' is not defined"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris_X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_Y = pd.Series(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''jak wygląda zbiór danych?'''\n",
    "\n",
    "iris_X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_Y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_Y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do celów edukacyjnych: zamieńmy zmienną zależną/ odpowiedzi \"Y\" na postać one-hot encoding, zobaczymy jak zachowuje się funkcja softmax\n",
    "\n",
    "iris_Y_one_hot = tf.keras.utils.to_categorical(iris_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''podział danych na uczące i treningowe'''\n",
    "\n",
    "# zazwyczaj robi się to kilka razy, w ramach procedury \"walidacji krzyżowej\" (ang. cross validation). Dla celów edukacyjnych,\n",
    "# w naszym przypadku dokonamy jednokrotnego podziału na zbiór uczący i testowy\n",
    "\n",
    "iris_X_train, iris_X_test, iris_Y_train, iris_Y_test = train_test_split(iris_X, iris_Y_one_hot, train_size=0.8, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X_train.shape, iris_Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''normalizacja danych'''\n",
    "\n",
    "# sieć neuronowa jest podatna na \"wysycenie gradientów\" (ang. gradient vanishing), tj. sytuację w której duże wartości\n",
    "# wpływają na szybkie odiągnięcie maksimum przez funkcję aktywacji. \n",
    "# W tym celu normalizujemy dane z użyciem uczącego się algorytmu skalującego (ang. scaler)\n",
    "# Uczymy \"StandardScaler na danych treningoeych, a potem przenosimy tę wiedzę na dane testowe,\n",
    "# żeby oddać rzeczywistą sytuację\n",
    "\n",
    "scaler = StandardScaler()\n",
    "iris_X_trains = scaler.fit_transform(iris_X_train)\n",
    "iris_X_trains = pd.DataFrame(iris_X_trains, columns=iris_X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X_trains.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pierwsza sieć neuronowa'''\n",
    "\n",
    "# sieć z użyciem Tensorflow - będzie się składać z 2 warstw:\n",
    "# 1. Warstwy ukrytej - przyjmującej cechy opisujące kwiaty irysa\n",
    "# 2. Warstwy wyjściowej - dokonującej klasyfikacji na 3 klasy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = iris_Y_one_hot.shape[1] # liczba klas\n",
    "nx = iris_X_train.shape[1] # liczba cech\n",
    "nh = 8 # liczba neuronów w warstwie ukrytej\n",
    "\n",
    "W1 = tf.Variable(np.random.randn(nx, nh), name='W1')\n",
    "b1 = tf.Variable(np.random.randn(1, nh), name='b1')\n",
    "\n",
    "W2 = tf.Variable(np.random.randn(nh, no), name='W2')\n",
    "b2 = tf.Variable(np.random.randn(1, no), name='b2')\n",
    "\n",
    "lossf = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batches = 6\n",
    "beta = 0.001\n",
    "nbatch = iris_X_train.shape[0]//batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "for e in range(epochs):\n",
    "    iris_X_trains, iris_Y_train = shuffle(iris_X_trains, iris_Y_train)\n",
    "    offset = 0 \n",
    "    for b in range(batches):\n",
    "        end = offset + nbatch\n",
    "        X, Y = iris_X_trains.iloc[offset:end, :], iris_Y_train[offset:end, :]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            h1 = tf.matmul(X, W1) + b1\n",
    "            z1 = tf.nn.relu(h1)\n",
    "\n",
    "            h2 = tf.matmul(z1, W2) + b2\n",
    "            yhat = tf.nn.softmax(h2)\n",
    "\n",
    "           \n",
    "            loss = lossf(Y, yhat)\n",
    "            reg = beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(b2))\n",
    "            total_loss = loss + reg\n",
    "\n",
    "            losses.append(total_loss.numpy())\n",
    "\n",
    "            acc = tf.keras.metrics.categorical_accuracy(Y, yhat)\n",
    "            accuracies.append(np.mean(acc.numpy()))\n",
    "\n",
    "        gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "        optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
    "        offset = end - 1\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Funkcja kosztu w czasie dla prostej sieci\")\n",
    "plt.xlabel(\"Numer iteracji\")\n",
    "plt.ylabel(\"Wartość funkcji kosztu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies)\n",
    "plt.title(\"Dokładność w czasie dla prostej sieci\")\n",
    "plt.xlabel(\"Numer iteracji\")\n",
    "plt.ylabel(\"Dokładność\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Wykorzystanie gotowej biblioteki'''\n",
    "\n",
    "# oczywiście przedstawiony wyżej sposób tworzenia sieci ma wyłącznie charakter podglądowy\n",
    "\n",
    "'''Kolejność działań jest następująca:\n",
    "    1. Zainicjujmy obiekt klasy 'Sequential' - jest to pusty obiekt sieci neuronowej, w której kolejne warstwy są układane jedna po drugiej.\n",
    "    2. Następnie za pomocą metody 'add' dodajemy kolejne warstwy ukryte i końcową, o ilości neuronów równej ilości klas do przewidzenia.\n",
    "    3. W zwykłej sieci klasyfikacyjnej (Multi Layer Perceptron) podstawę architektury stanowi wartwa typu 'Dense').\n",
    "    4. W pierwszej warstwie należy wyspecyfikować:\n",
    "        - 'input_dim' czyli liczbę atrybutów/ cech obiektów (wierszy) danych\n",
    "        - liczbę neuronów 'units' \n",
    "        - funkcję aktywacji 'activation'\n",
    "    5. Po zdefiniowaniu sieci kompilujemy ją za pomocą metody 'compile', w której należy podać:\n",
    "        - optimizer - algorytm optymalizacyjny, którym będzie się posługiwać sieć\n",
    "        - loss - funkcję kosztu, którą sieć będzie minimalizować\n",
    "        - metrics - metrykę, którą będziemy monitorować w czasie uczenia\n",
    "        \n",
    "    Zreplikujemy sieć, którą wcześniej napisaliśmy ręcznie, żeby porównać wyniki'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Zreplikujemy sieć, którą wcześniej napisaliśmy ręcznie, żeby porównać wyniki'''\n",
    "\n",
    "nh, nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(input_dim=nx, units=nh, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=no, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(iris_X_trains, iris_Y_train, epochs=15, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Funkcja kosztu, gdy używamy biblioteki\")\n",
    "plt.xlabel(\"Iteracja\")\n",
    "plt.ylabel(\"Koszt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.title(\"Trafność, gdy używamy biblioteki\")\n",
    "plt.xlabel(\"Iteracja\")\n",
    "plt.ylabel(\"Trafność\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predykcja na zbiorze testowym z wykorzystaniem gotowej biblioteki'''\n",
    "\n",
    "# teraz dokonamy predykcjina zbiorze testowym, żeby ocenić trafność klasyfikacji naszego modelu.\n",
    "# ważne jest, aby dokonać normalizacji z użyciem 'Scalera' wyuczonego na zbiorze treningowym\n",
    "\n",
    "iris_X_test_scaled = scaler.transform(iris_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_iris_test = model.predict(iris_X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tf.metrics.categorical_accuracy(iris_Y_test, yhat_iris_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Przykład 2 - Prognozowanie wartości numerycznych (cen mieszkań)\n",
    "\n",
    "Klasyfikacja jest przedstawiana jako klasyczny przykład zastosowania prostych sieci neuronowych.\n",
    "Innym jest regresja, czyli prognozowanie wartości numerycznych. Posłużymy się zbiorem danych 'Boston housing',\n",
    "gdzie na podstawiec ceh mieszkania będziemy prognozować jego cenę.\n",
    "\n",
    "1. Za pomocą pakietu scikit-learn załadujemy dane ze zbioru Boston housing.\n",
    "2. Przekonwertujemy je do postaci Data Frame, żeby móc wygodnie operować na danych.\n",
    "3. Przprowadzimy podział na zbiór treningowy i testowy.\n",
    "4. Przprowadzimy normalizację danych.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytanie i wstępna obróbka\n",
    "boston = load_boston()\n",
    "boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "boston_y = pd.Series(boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podział na zbiór testowy i uczący\n",
    "\n",
    "boston_X_train, boston_X_test, boston_y_train, boston_y_test = train_test_split(boston_df, boston_y, train_size=0.8, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_X_train.shape, boston_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizacja danych\n",
    "scaler_boston = StandardScaler()\n",
    "boston_X_trains = scaler_boston.fit_transform(boston_X_train)\n",
    "boston_X_trains = pd.DataFrame(boston_X_trains, columns=boston_X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Wykorzystanie biblioteki do zbudowania sieci\n",
    "\n",
    "Kroki budowy sieci z użyciem biblioteki są analogiczne jak w przypadku klasyfikacji. Jest kilka różnic:\n",
    "1. W tym przypadku, jako aktywacji ostatniej warstwy użyjemu ReLu, ponieważ prognozujemy wartości numeryczne.\n",
    "2. Funkcją kosztu będzie Mean Squared Error (MSE), ponieważ mamy do czynienia z regresją.\n",
    "3. Dodamy (jako podgląd) średni bezwzględny błąd procentowy (MAPE), żeby wiedzieć o ile % ceny docelowej myli się nasz model.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_boston = boston_X_train.shape[1] # liczba cech\n",
    "nh_boston = 12 # liczba neuronów w warstwie ukrytej\n",
    "\n",
    "model_boston = tf.keras.models.Sequential()\n",
    "model_boston.add(tf.keras.layers.Dense(input_dim=nx_boston, units=nh_boston, activation='relu'))\n",
    "model_boston.add(tf.keras.layers.Dense(units=1, activation='relu'))\n",
    "model_boston.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(momentum=0.9), \n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    metrics=[tf.keras.losses.MeanSquaredError(name=\"MSE\"), tf.keras.losses.MeanAbsolutePercentageError(name=\"MAPE\")])\n",
    "\n",
    "print(model_boston.summary())\n",
    "tf.keras.utils.plot_model(model_boston, show_shapes=True, show_layer_names=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_boston = model_boston.fit(boston_X_trains, boston_y_train, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_boston.history[\"loss\"])\n",
    "plt.title(\"Błąd średniokwadratowy cen\")\n",
    "plt.xlabel(\"Iteracja\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_boston.history[\"MSE\"])\n",
    "plt.title(\"Błąd [%] cen\")\n",
    "plt.xlabel(\"Iteracja\")\n",
    "plt.ylabel(\"MAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predykcja na zbiorze testowym\n",
    "\n",
    "Podobnie jako to miało miejsce, w przypadku klasyfikacji kwiatów irysa - dokonamy predykcji na zbiorze testowym.\n",
    "Najpierw należy znormalizować dane z wykorzystaniem 'Scalera' nauczonego na danych treningowych'''\n",
    "\n",
    "boston_X_test_scaled = scaler_boston.transform(boston_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_boston = model_boston.predict(boston_X_test_scaled).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(boston_y_test, yhat_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.mean_absolute_percentage_error(boston_y_test, yhat_boston).numpy()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
